{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/fotimakhongulomova/scholarai-project-for-genai-course?scriptVersionId=238518159\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","id":"e94c22b9","metadata":{"papermill":{"duration":0.00627,"end_time":"2025-05-08T08:15:13.962816","exception":false,"start_time":"2025-05-08T08:15:13.956546","status":"completed"},"tags":[]},"source":["# ScholarAI: An Intelligent AI Agent for Personalized Learning\n","\n","ScholarAI is a generative AI-powered learning assistant designed to help students study more effectively using their own materials. Built using LangChain v0.1+, Gemini, and modern GenAI capabilities, ScholarAI enables personalized, grounded, and interactive academic support.\n","\n","In this notebook, I demonstrate how ScholarAI can:\n","\n","âœ… Summarize research papers into concise study notes\n","\n","âœ… Answer personalized questions based on user-uploaded content using RAG (Retrieval-Augmented Generation)\n","\n","This capstone project was developed as part of the 5-Day GenAI Intensive Course by Google and Kaggle, and highlights practical use of few-shot prompting, retrieval-based Q&A, and function calling in a real-world educational scenario.\n","\n","**Presentation of the Project:** https://gamma.app/docs/ScholarAI-Academic-Assistant-with-Summarization-and-RAG-QA-bgbgzr35625kdy0"]},{"cell_type":"markdown","id":"0b4c0a16","metadata":{"papermill":{"duration":0.004966,"end_time":"2025-05-08T08:15:13.973284","exception":false,"start_time":"2025-05-08T08:15:13.968318","status":"completed"},"tags":[]},"source":["## Features Used in ScholarAI Project\n","The following GenAI features from the list were successfully implemented:\n","\n","* **Few-shot prompting** â€“ Custom prompt templates were used to guide the summarization and Q&A responses.\n","* **Document understanding** â€“ Academic texts were split, embedded, and processed using LangChain's document tools.\n","* **Embeddings** â€“ Text chunks were converted into embeddings using `GoogleGenerativeAIEmbeddings`.\n","* **Retrieval-Augmented Generation (RAG)** â€“ The core system retrieves relevant documents before generating answers.\n","* **Vector search/vector store/vector database** â€“ Used `InMemoryVectorStore` for similarity-based retrieval of academic content."]},{"cell_type":"markdown","id":"778f898f","metadata":{"papermill":{"duration":0.005909,"end_time":"2025-05-08T08:15:13.98417","exception":false,"start_time":"2025-05-08T08:15:13.978261","status":"completed"},"tags":[]},"source":["## Diagram for ScholarAI\n","\n","![ScholarAI Diagram](https://i.postimg.cc/KcMBr9Qv/Diagram-for-Scholar-AI.png)"]},{"cell_type":"markdown","id":"fe1af378","metadata":{"papermill":{"duration":0.004643,"end_time":"2025-05-08T08:15:13.993515","exception":false,"start_time":"2025-05-08T08:15:13.988872","status":"completed"},"tags":[]},"source":["## Notebook Imports"]},{"cell_type":"code","execution_count":1,"id":"e38a89f9","metadata":{"execution":{"iopub.execute_input":"2025-05-08T08:15:14.004464Z","iopub.status.busy":"2025-05-08T08:15:14.004166Z","iopub.status.idle":"2025-05-08T08:15:41.693881Z","shell.execute_reply":"2025-05-08T08:15:41.692986Z"},"papermill":{"duration":27.697357,"end_time":"2025-05-08T08:15:41.695607","exception":false,"start_time":"2025-05-08T08:15:13.99825","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[33mWARNING: Skipping kfp as it is not installed.\u001b[0m\u001b[33m\r\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m138.0/138.0 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m437.7/437.7 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m47.6/47.6 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m223.6/223.6 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25h"]}],"source":["# Remove conflicting packages from the Kaggle base environment.\n","!pip uninstall -qqy kfp jupyterlab libpysal thinc spacy fastai ydata-profiling google-cloud-bigquery google-generativeai\n","# Install langgraph and the packages used in this lab.\n","!pip install -qU 'langgraph==0.3.21' 'langchain-google-genai==2.1.2' 'langgraph-prebuilt==0.1.7'"]},{"cell_type":"code","execution_count":2,"id":"b7cb5630","metadata":{"execution":{"iopub.execute_input":"2025-05-08T08:15:41.708641Z","iopub.status.busy":"2025-05-08T08:15:41.708339Z","iopub.status.idle":"2025-05-08T08:15:51.270111Z","shell.execute_reply":"2025-05-08T08:15:51.268991Z"},"papermill":{"duration":9.570191,"end_time":"2025-05-08T08:15:51.271887","exception":false,"start_time":"2025-05-08T08:15:41.701696","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m65.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25h"]}],"source":["!pip install -qU langchain langchain-community"]},{"cell_type":"code","execution_count":3,"id":"6425f94f","metadata":{"execution":{"iopub.execute_input":"2025-05-08T08:15:51.285546Z","iopub.status.busy":"2025-05-08T08:15:51.285222Z","iopub.status.idle":"2025-05-08T08:15:54.952161Z","shell.execute_reply":"2025-05-08T08:15:54.950828Z"},"papermill":{"duration":3.675964,"end_time":"2025-05-08T08:15:54.95418","exception":false,"start_time":"2025-05-08T08:15:51.278216","status":"completed"},"tags":[]},"outputs":[],"source":["!pip install -qU langchain-core"]},{"cell_type":"code","execution_count":4,"id":"b7e9cc50","metadata":{"execution":{"iopub.execute_input":"2025-05-08T08:15:54.967081Z","iopub.status.busy":"2025-05-08T08:15:54.966754Z","iopub.status.idle":"2025-05-08T08:15:59.600044Z","shell.execute_reply":"2025-05-08T08:15:59.599266Z"},"papermill":{"duration":4.641616,"end_time":"2025-05-08T08:15:59.60174","exception":false,"start_time":"2025-05-08T08:15:54.960124","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:623: UserWarning: <built-in function any> is not a Python type (it may be an instance of an object), Pydantic will allow any object with no validation since we cannot even enforce that the input is an instance of the given type. To get rid of this error wrap the type with `pydantic.SkipValidation`.\n","  warn(\n"]}],"source":["# General Python Libraries\n","import os\n","import pandas as pd\n","from typing_extensions import List, TypedDict\n","\n","# Kaggle Secrets\n","from kaggle_secrets import UserSecretsClient\n","\n","# LangChain Core\n","from langchain_core.documents import Document\n","from langchain_core.prompts import PromptTemplate\n","from langchain_core.runnables import RunnableLambda\n","from langchain_core.vectorstores import InMemoryVectorStore\n","from langchain.chains.llm import LLMChain\n","from langchain.chains.combine_documents import create_stuff_documents_chain\n","from langchain import FewShotPromptTemplate\n","\n","# LangChain Utilities\n","from langchain_text_splitters import CharacterTextSplitter, RecursiveCharacterTextSplitter\n","from langchain_community.document_loaders import PyPDFLoader\n","\n","# LangChain + Gemini\n","from langchain_google_genai import (\n","    ChatGoogleGenerativeAI,\n","    GoogleGenerativeAI,\n","    GoogleGenerativeAIEmbeddings\n",")\n","\n","# Gemini Native SDK (optional)\n","from google import genai\n","from google.genai import types"]},{"cell_type":"code","execution_count":5,"id":"6304918e","metadata":{"execution":{"iopub.execute_input":"2025-05-08T08:15:59.615332Z","iopub.status.busy":"2025-05-08T08:15:59.61482Z","iopub.status.idle":"2025-05-08T08:15:59.780098Z","shell.execute_reply":"2025-05-08T08:15:59.77936Z"},"papermill":{"duration":0.173561,"end_time":"2025-05-08T08:15:59.781649","exception":false,"start_time":"2025-05-08T08:15:59.608088","status":"completed"},"tags":[]},"outputs":[],"source":["GOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\n","os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n","\n","client = genai.Client(api_key=GOOGLE_API_KEY)"]},{"cell_type":"markdown","id":"e2829679","metadata":{"papermill":{"duration":0.005778,"end_time":"2025-05-08T08:15:59.793487","exception":false,"start_time":"2025-05-08T08:15:59.787709","status":"completed"},"tags":[]},"source":["## ğŸ“ Document Summarizer\n","\n","The Document Summarizer enables students to upload learning materials (such as class notes, articles, or textbooks) and instantly receive a concise summary."]},{"cell_type":"code","execution_count":6,"id":"1d2b8482","metadata":{"execution":{"iopub.execute_input":"2025-05-08T08:15:59.806388Z","iopub.status.busy":"2025-05-08T08:15:59.806116Z","iopub.status.idle":"2025-05-08T08:16:01.656292Z","shell.execute_reply":"2025-05-08T08:16:01.655416Z"},"papermill":{"duration":1.858239,"end_time":"2025-05-08T08:16:01.657679","exception":false,"start_time":"2025-05-08T08:15:59.79944","status":"completed"},"tags":[]},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>titles</th>\n","      <th>summaries</th>\n","      <th>terms</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Survey on Semantic Stereo Matching / Semantic ...</td>\n","      <td>Stereo matching is one of the widely used tech...</td>\n","      <td>['cs.CV', 'cs.LG']</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>FUTURE-AI: Guiding Principles and Consensus Re...</td>\n","      <td>The recent advancements in artificial intellig...</td>\n","      <td>['cs.CV', 'cs.AI', 'cs.LG']</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Enforcing Mutual Consistency of Hard Regions f...</td>\n","      <td>In this paper, we proposed a novel mutual cons...</td>\n","      <td>['cs.CV', 'cs.AI']</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Parameter Decoupling Strategy for Semi-supervi...</td>\n","      <td>Consistency training has proven to be an advan...</td>\n","      <td>['cs.CV']</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Background-Foreground Segmentation for Interio...</td>\n","      <td>To ensure safety in automated driving, the cor...</td>\n","      <td>['cs.CV', 'cs.LG']</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                              titles  \\\n","0  Survey on Semantic Stereo Matching / Semantic ...   \n","1  FUTURE-AI: Guiding Principles and Consensus Re...   \n","2  Enforcing Mutual Consistency of Hard Regions f...   \n","3  Parameter Decoupling Strategy for Semi-supervi...   \n","4  Background-Foreground Segmentation for Interio...   \n","\n","                                           summaries  \\\n","0  Stereo matching is one of the widely used tech...   \n","1  The recent advancements in artificial intellig...   \n","2  In this paper, we proposed a novel mutual cons...   \n","3  Consistency training has proven to be an advan...   \n","4  To ensure safety in automated driving, the cor...   \n","\n","                         terms  \n","0           ['cs.CV', 'cs.LG']  \n","1  ['cs.CV', 'cs.AI', 'cs.LG']  \n","2           ['cs.CV', 'cs.AI']  \n","3                    ['cs.CV']  \n","4           ['cs.CV', 'cs.LG']  "]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["data1_path = '/kaggle/input/arxiv-paper-abstracts/arxiv_data.csv'\n","data2_path = '/kaggle/input/arxiv-paper-abstracts/arxiv_data_210930-054931.csv'\n","\n","df1 = pd.read_csv(data1_path)\n","df1.head()"]},{"cell_type":"code","execution_count":7,"id":"c373158a","metadata":{"execution":{"iopub.execute_input":"2025-05-08T08:16:01.6719Z","iopub.status.busy":"2025-05-08T08:16:01.671124Z","iopub.status.idle":"2025-05-08T08:16:03.561573Z","shell.execute_reply":"2025-05-08T08:16:03.560742Z"},"papermill":{"duration":1.899283,"end_time":"2025-05-08T08:16:03.563296","exception":false,"start_time":"2025-05-08T08:16:01.664013","status":"completed"},"tags":[]},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>terms</th>\n","      <th>titles</th>\n","      <th>summaries</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>56176</th>\n","      <td>['cs.CV', 'cs.IR']</td>\n","      <td>Mining Spatio-temporal Data on Industrializati...</td>\n","      <td>Despite the growing availability of big data i...</td>\n","    </tr>\n","    <tr>\n","      <th>56177</th>\n","      <td>['cs.LG', 'cs.AI', 'cs.CL', 'I.2.6; I.2.7']</td>\n","      <td>Wav2Letter: an End-to-End ConvNet-based Speech...</td>\n","      <td>This paper presents a simple end-to-end model ...</td>\n","    </tr>\n","    <tr>\n","      <th>56178</th>\n","      <td>['cs.LG']</td>\n","      <td>Deep Reinforcement Learning with Double Q-lear...</td>\n","      <td>The popular Q-learning algorithm is known to o...</td>\n","    </tr>\n","    <tr>\n","      <th>56179</th>\n","      <td>['stat.ML', 'cs.LG', 'math.OC']</td>\n","      <td>Generalized Low Rank Models</td>\n","      <td>Principal components analysis (PCA) is a well-...</td>\n","    </tr>\n","    <tr>\n","      <th>56180</th>\n","      <td>['cs.LG', 'cs.AI', 'stat.ML']</td>\n","      <td>Chi-square Tests Driven Method for Learning th...</td>\n","      <td>SDYNA is a general framework designed to addre...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                             terms  \\\n","56176                           ['cs.CV', 'cs.IR']   \n","56177  ['cs.LG', 'cs.AI', 'cs.CL', 'I.2.6; I.2.7']   \n","56178                                    ['cs.LG']   \n","56179              ['stat.ML', 'cs.LG', 'math.OC']   \n","56180                ['cs.LG', 'cs.AI', 'stat.ML']   \n","\n","                                                  titles  \\\n","56176  Mining Spatio-temporal Data on Industrializati...   \n","56177  Wav2Letter: an End-to-End ConvNet-based Speech...   \n","56178  Deep Reinforcement Learning with Double Q-lear...   \n","56179                        Generalized Low Rank Models   \n","56180  Chi-square Tests Driven Method for Learning th...   \n","\n","                                               summaries  \n","56176  Despite the growing availability of big data i...  \n","56177  This paper presents a simple end-to-end model ...  \n","56178  The popular Q-learning algorithm is known to o...  \n","56179  Principal components analysis (PCA) is a well-...  \n","56180  SDYNA is a general framework designed to addre...  "]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["df2 = pd.read_csv(data2_path)\n","df2['summaries'] = df2['abstracts']\n","df2 = df2.drop(\"abstracts\", axis='columns')\n","df2.tail()"]},{"cell_type":"code","execution_count":8,"id":"c15d3c58","metadata":{"execution":{"iopub.execute_input":"2025-05-08T08:16:03.579261Z","iopub.status.busy":"2025-05-08T08:16:03.578943Z","iopub.status.idle":"2025-05-08T08:16:03.593947Z","shell.execute_reply":"2025-05-08T08:16:03.59311Z"},"papermill":{"duration":0.023559,"end_time":"2025-05-08T08:16:03.595398","exception":false,"start_time":"2025-05-08T08:16:03.571839","status":"completed"},"tags":[]},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>titles</th>\n","      <th>summaries</th>\n","      <th>terms</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Survey on Semantic Stereo Matching / Semantic ...</td>\n","      <td>Stereo matching is one of the widely used tech...</td>\n","      <td>['cs.CV', 'cs.LG']</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>FUTURE-AI: Guiding Principles and Consensus Re...</td>\n","      <td>The recent advancements in artificial intellig...</td>\n","      <td>['cs.CV', 'cs.AI', 'cs.LG']</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Enforcing Mutual Consistency of Hard Regions f...</td>\n","      <td>In this paper, we proposed a novel mutual cons...</td>\n","      <td>['cs.CV', 'cs.AI']</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Parameter Decoupling Strategy for Semi-supervi...</td>\n","      <td>Consistency training has proven to be an advan...</td>\n","      <td>['cs.CV']</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Background-Foreground Segmentation for Interio...</td>\n","      <td>To ensure safety in automated driving, the cor...</td>\n","      <td>['cs.CV', 'cs.LG']</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                              titles  \\\n","0  Survey on Semantic Stereo Matching / Semantic ...   \n","1  FUTURE-AI: Guiding Principles and Consensus Re...   \n","2  Enforcing Mutual Consistency of Hard Regions f...   \n","3  Parameter Decoupling Strategy for Semi-supervi...   \n","4  Background-Foreground Segmentation for Interio...   \n","\n","                                           summaries  \\\n","0  Stereo matching is one of the widely used tech...   \n","1  The recent advancements in artificial intellig...   \n","2  In this paper, we proposed a novel mutual cons...   \n","3  Consistency training has proven to be an advan...   \n","4  To ensure safety in automated driving, the cor...   \n","\n","                         terms  \n","0           ['cs.CV', 'cs.LG']  \n","1  ['cs.CV', 'cs.AI', 'cs.LG']  \n","2           ['cs.CV', 'cs.AI']  \n","3                    ['cs.CV']  \n","4           ['cs.CV', 'cs.LG']  "]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["df = pd.concat([df1, df2], ignore_index=True)\n","df.head()"]},{"cell_type":"code","execution_count":9,"id":"05efbcd6","metadata":{"execution":{"iopub.execute_input":"2025-05-08T08:16:03.609797Z","iopub.status.busy":"2025-05-08T08:16:03.609343Z","iopub.status.idle":"2025-05-08T08:16:03.622327Z","shell.execute_reply":"2025-05-08T08:16:03.621661Z"},"papermill":{"duration":0.021513,"end_time":"2025-05-08T08:16:03.623647","exception":false,"start_time":"2025-05-08T08:16:03.602134","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["(1080, 3)"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["df = df.sample(frac=0.01, random_state=42).reset_index().drop(\"index\", axis='columns')\n","df.shape"]},{"cell_type":"code","execution_count":10,"id":"e3c38a89","metadata":{"execution":{"iopub.execute_input":"2025-05-08T08:16:03.638335Z","iopub.status.busy":"2025-05-08T08:16:03.637705Z","iopub.status.idle":"2025-05-08T08:16:03.643038Z","shell.execute_reply":"2025-05-08T08:16:03.642133Z"},"papermill":{"duration":0.014089,"end_time":"2025-05-08T08:16:03.644388","exception":false,"start_time":"2025-05-08T08:16:03.630299","status":"completed"},"tags":[]},"outputs":[],"source":["def get_text(dataframe: pd.DataFrame, max_count: int = 2) -> str:\n","    entries = []\n","    for i, row in dataframe[['titles', 'summaries']].dropna().head(max_count).iterrows():\n","        entry = f\"Title: {row['titles']}\\nSummary: {row['summaries']}\"\n","        entries.append(entry)\n","    return \"\\n\\n\".join(entries)"]},{"cell_type":"code","execution_count":11,"id":"9104d91b","metadata":{"execution":{"iopub.execute_input":"2025-05-08T08:16:03.658587Z","iopub.status.busy":"2025-05-08T08:16:03.658328Z","iopub.status.idle":"2025-05-08T08:16:03.663872Z","shell.execute_reply":"2025-05-08T08:16:03.663267Z"},"papermill":{"duration":0.014408,"end_time":"2025-05-08T08:16:03.665254","exception":false,"start_time":"2025-05-08T08:16:03.650846","status":"completed"},"tags":[]},"outputs":[],"source":["def text_summarizer(text):\n","    # Step 1: Split text into chunks\n","    text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n","        chunk_size=500,\n","        chunk_overlap=50,\n","    )\n","    docs = [Document(page_content=text)]\n","    split_docs = text_splitter.split_documents(docs)\n","\n","    # Step 2: Set up Gemini via LangChain GoogleGenerativeAI\n","    llm = GoogleGenerativeAI(\n","        model=\"models/gemini-2.0-flash\",\n","        google_api_key=GOOGLE_API_KEY,\n","        temperature=0.1\n","    )\n","\n","    # Step 3: Use context as the input variable\n","    prompt = PromptTemplate(\n","        input_variables=[\"context\"],\n","        template=(\n","            \"You are an expert academic summarizer.\\n\"\n","            \"Summarize the following academic research papers into concise paragraphs:\\n\\n\"\n","            \"{context}\\n\\n\"\n","            \"Summary:\"\n","        )\n","    )\n","\n","    # Step 4: Create the chain\n","    chain = create_stuff_documents_chain(llm, prompt)\n","\n","    # Step 5: Run the chain\n","    result = chain.invoke({\"context\": split_docs})\n","\n","    return result"]},{"cell_type":"code","execution_count":12,"id":"019a079e","metadata":{"execution":{"iopub.execute_input":"2025-05-08T08:16:03.67953Z","iopub.status.busy":"2025-05-08T08:16:03.67926Z","iopub.status.idle":"2025-05-08T08:16:03.687474Z","shell.execute_reply":"2025-05-08T08:16:03.686609Z"},"papermill":{"duration":0.016875,"end_time":"2025-05-08T08:16:03.688748","exception":false,"start_time":"2025-05-08T08:16:03.671873","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["'Title: A Multi-Object Rectified Attention Network for Scene Text Recognition\\nSummary: Irregular text is widely used. However, it is considerably difficult to\\nrecognize because of its various shapes and distorted patterns. In this paper,\\nwe thus propose a multi-object rectified attention network (MORAN) for general\\nscene text recognition. The MORAN consists of a multi-object rectification\\nnetwork and an attention-based sequence recognition network. The multi-object\\nrectification network is designed for rectifying images that contain irregular\\ntext. It decreases the difficulty of recognition and enables the\\nattention-based sequence recognition network to more easily read irregular\\ntext. It is trained in a weak supervision way, thus requiring only images and\\ncorresponding text labels. The attention-based sequence recognition network\\nfocuses on target characters and sequentially outputs the predictions.\\nMoreover, to improve the sensitivity of the attention-based sequence\\nrecognition network, a fractional pickup method is proposed for an\\nattention-based decoder in the training phase. With the rectification\\nmechanism, the MORAN can read both regular and irregular scene text. Extensive\\nexperiments on various benchmarks are conducted, which show that the MORAN\\nachieves state-of-the-art performance. The source code is available.\\n\\nTitle: Grounding Human-to-Vehicle Advice for Self-driving Vehicles\\nSummary: Recent success suggests that deep neural control networks are likely to be a\\nkey component of self-driving vehicles. These networks are trained on large\\ndatasets to imitate human actions, but they lack semantic understanding of\\nimage contents. This makes them brittle and potentially unsafe in situations\\nthat do not match training data. Here, we propose to address this issue by\\naugmenting training data with natural language advice from a human. Advice\\nincludes guidance about what to do and where to attend. We present the first\\nstep toward advice giving, where we train an end-to-end vehicle controller that\\naccepts advice. The controller adapts the way it attends to the scene (visual\\nattention) and the control (steering and speed). Attention mechanisms tie\\ncontroller behavior to salient objects in the advice. We evaluate our model on\\na novel advisable driving dataset with manually annotated human-to-vehicle\\nadvice called Honda Research Institute-Advice Dataset (HAD). We show that\\ntaking advice improves the performance of the end-to-end network, while the\\nnetwork cues on a variety of visual features that are provided by advice. The\\ndataset is available at https://usa.honda-ri.com/HAD.'"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["text = get_text(df)\n","text"]},{"cell_type":"code","execution_count":13,"id":"727793cc","metadata":{"execution":{"iopub.execute_input":"2025-05-08T08:16:03.703355Z","iopub.status.busy":"2025-05-08T08:16:03.703067Z","iopub.status.idle":"2025-05-08T08:16:08.635994Z","shell.execute_reply":"2025-05-08T08:16:08.634963Z"},"papermill":{"duration":4.941911,"end_time":"2025-05-08T08:16:08.637588","exception":false,"start_time":"2025-05-08T08:16:03.695677","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["**A Multi-Object Rectified Attention Network for Scene Text Recognition:** This paper introduces a Multi-Object Rectified Attention Network (MORAN) designed to improve scene text recognition, particularly for irregular text. MORAN combines a multi-object rectification network, which corrects distorted text images to ease recognition, with an attention-based sequence recognition network that focuses on relevant characters for sequential prediction. A fractional pickup method is also introduced to enhance the sensitivity of the attention-based decoder during training. The model is trained with weak supervision, requiring only images and text labels, and demonstrates state-of-the-art performance on various benchmarks for both regular and irregular text.\n","\n","**Grounding Human-to-Vehicle Advice for Self-driving Vehicles:** This research addresses the limitations of deep neural control networks in self-driving vehicles by incorporating natural language advice from humans. The proposed approach trains an end-to-end vehicle controller that integrates advice to adjust its visual attention and control (steering and speed). Attention mechanisms link controller behavior to salient objects mentioned in the advice. Evaluated on the Honda Research Institute-Advice Dataset (HAD), the results demonstrate that incorporating advice improves the performance of the end-to-end network, enabling it to focus on visual features indicated by the advice.\n"]}],"source":["summary = text_summarizer(text=text)\n","print(summary)"]},{"cell_type":"markdown","id":"b915af23","metadata":{"papermill":{"duration":0.006657,"end_time":"2025-05-08T08:16:08.651607","exception":false,"start_time":"2025-05-08T08:16:08.64495","status":"completed"},"tags":[]},"source":["## ğŸ” Personalized Q&A (RAG system)\n","This tool enables students to ask questions grounded in their own study materials using a Retrieval-Augmented Generation (RAG) pipeline. "]},{"cell_type":"code","execution_count":14,"id":"10f3af87","metadata":{"execution":{"iopub.execute_input":"2025-05-08T08:16:08.666504Z","iopub.status.busy":"2025-05-08T08:16:08.666203Z","iopub.status.idle":"2025-05-08T08:16:08.67183Z","shell.execute_reply":"2025-05-08T08:16:08.670957Z"},"papermill":{"duration":0.014824,"end_time":"2025-05-08T08:16:08.673253","exception":false,"start_time":"2025-05-08T08:16:08.658429","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["'Title: A Multi-Object Rectified Attention Network for Scene Text Recognition\\nSummary: Irregular text is widely used. However, it is considerably difficult to\\nrecognize because of its various shapes and distorted patterns. In this paper,\\nwe thus propose a multi-object rectified attention network (MORAN) for general\\nscene text recognition. The MORAN consists of a multi-object rectification\\nnetwork and an attention-based sequence recognition network. The multi-object\\nrectification network is designed for rectifying images that contain irregular\\ntext. It decreases the difficulty of recognition and enables the\\nattention-based sequence recognition network to more easily read irregular\\ntext. It is trained in a weak supervision way, thus requiring only images and\\ncorresponding text labels. The attention-based sequence recognition network\\nfocuses on target characters and sequentially outputs the predictions.\\nMoreover, to improve the sensitivity of the attention-based sequence\\nrecognition network, a fractional pickup method is proposed for an\\nattention-based decoder in the training phase. With the rectification\\nmechanism, the MORAN can read both regular and irregular scene text. Extensive\\nexperiments on various benchmarks are conducted, which show that the MORAN\\nachieves state-of-the-art performance. The source code is available.\\n\\nTitle: Grounding Human-to-Vehicle Advice for Self-driving Vehicles\\nSummary: Recent success suggests that deep neural control networks are likely to be a\\nkey component of self-driving vehicles. These networks are trained on large\\ndatasets to imitate human actions, but they lack semantic understanding of\\nimage contents. This makes them brittle and potentially unsafe in situations\\nthat do not match training data. Here, we propose to address this issue by\\naugmenting training data with natural language advice from a human. Advice\\nincludes guidance about what to do and where to attend. We present the first\\nstep toward advice giving, where we train an end-to-end vehicle controller that\\naccepts advice. The controller adapts the way it attends to the scene (visual\\nattention) and the control (steering and speed). Attention mechanisms tie\\ncontroller behavior to salient objects in the advice. We evaluate our model on\\na novel advisable driving dataset with manually annotated human-to-vehicle\\nadvice called Honda Research Institute-Advice Dataset (HAD). We show that\\ntaking advice improves the performance of the end-to-end network, while the\\nnetwork cues on a variety of visual features that are provided by advice. The\\ndataset is available at https://usa.honda-ri.com/HAD.'"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["text"]},{"cell_type":"code","execution_count":15,"id":"a8a3aef2","metadata":{"execution":{"iopub.execute_input":"2025-05-08T08:16:08.688809Z","iopub.status.busy":"2025-05-08T08:16:08.688378Z","iopub.status.idle":"2025-05-08T08:16:08.694232Z","shell.execute_reply":"2025-05-08T08:16:08.69325Z"},"papermill":{"duration":0.015116,"end_time":"2025-05-08T08:16:08.695534","exception":false,"start_time":"2025-05-08T08:16:08.680418","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Split blog post into 4 sub-documents.\n"]}],"source":["# Splitting documents\n","text_splitter = RecursiveCharacterTextSplitter(\n","    chunk_size=1000,  # chunk size (characters)\n","    chunk_overlap=100,  # chunk overlap (characters)\n","    add_start_index=True,  # track index in original document\n",")\n","\n","docs = [Document(page_content=text)]\n","all_splits = text_splitter.split_documents(docs)\n","\n","print(f\"Split blog post into {len(all_splits)} sub-documents.\")"]},{"cell_type":"code","execution_count":16,"id":"36f61d3e","metadata":{"execution":{"iopub.execute_input":"2025-05-08T08:16:08.710367Z","iopub.status.busy":"2025-05-08T08:16:08.70979Z","iopub.status.idle":"2025-05-08T08:16:08.71593Z","shell.execute_reply":"2025-05-08T08:16:08.715315Z"},"papermill":{"duration":0.014701,"end_time":"2025-05-08T08:16:08.717135","exception":false,"start_time":"2025-05-08T08:16:08.702434","status":"completed"},"tags":[]},"outputs":[],"source":["embeddings = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\")"]},{"cell_type":"code","execution_count":17,"id":"b3605bb4","metadata":{"execution":{"iopub.execute_input":"2025-05-08T08:16:08.732517Z","iopub.status.busy":"2025-05-08T08:16:08.731608Z","iopub.status.idle":"2025-05-08T08:16:09.120045Z","shell.execute_reply":"2025-05-08T08:16:09.11923Z"},"papermill":{"duration":0.397415,"end_time":"2025-05-08T08:16:09.121459","exception":false,"start_time":"2025-05-08T08:16:08.724044","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["['d308fd3b-5b17-49f9-acf9-ea3888344240',\n"," '567cc2d9-1215-4ac9-a63d-d7ed6099e607',\n"," '7bb66e85-11c9-4d29-81d1-0304029c080c',\n"," '61538d15-32a6-4805-8be5-7b7ea81c9b4b']"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["vector_store = InMemoryVectorStore(embeddings)\n","vector_store.add_documents(all_splits)"]},{"cell_type":"code","execution_count":18,"id":"2f77073b","metadata":{"execution":{"iopub.execute_input":"2025-05-08T08:16:09.136534Z","iopub.status.busy":"2025-05-08T08:16:09.136039Z","iopub.status.idle":"2025-05-08T08:16:09.141947Z","shell.execute_reply":"2025-05-08T08:16:09.141304Z"},"papermill":{"duration":0.014803,"end_time":"2025-05-08T08:16:09.143202","exception":false,"start_time":"2025-05-08T08:16:09.128399","status":"completed"},"tags":[]},"outputs":[],"source":["llm = ChatGoogleGenerativeAI(\n","    model=\"models/gemini-2.0-flash\",\n","    temperature=0.2,\n","    google_api_key=GOOGLE_API_KEY\n",")"]},{"cell_type":"code","execution_count":19,"id":"331deed9","metadata":{"execution":{"iopub.execute_input":"2025-05-08T08:16:09.158184Z","iopub.status.busy":"2025-05-08T08:16:09.157873Z","iopub.status.idle":"2025-05-08T08:16:09.162279Z","shell.execute_reply":"2025-05-08T08:16:09.161657Z"},"papermill":{"duration":0.013165,"end_time":"2025-05-08T08:16:09.163405","exception":false,"start_time":"2025-05-08T08:16:09.15024","status":"completed"},"tags":[]},"outputs":[],"source":["template = \"\"\"Use the following pieces of context to answer the question at the end.\n","If you don't know the answer, just say that you don't know, don't try to make up an answer.\n","Use 4-5 sentences maximum and keep the answer as concise as possible.\n","\n","{context}\n","\n","Question: {question}\n","\n","Helpful Answer:\"\"\"\n","\n","prompt = PromptTemplate(\n","    input_variables=[\"question\", \"context\"],\n","    template=template\n",")"]},{"cell_type":"code","execution_count":20,"id":"6c715ce8","metadata":{"execution":{"iopub.execute_input":"2025-05-08T08:16:09.178066Z","iopub.status.busy":"2025-05-08T08:16:09.177772Z","iopub.status.idle":"2025-05-08T08:16:09.182152Z","shell.execute_reply":"2025-05-08T08:16:09.1813Z"},"papermill":{"duration":0.013144,"end_time":"2025-05-08T08:16:09.183496","exception":false,"start_time":"2025-05-08T08:16:09.170352","status":"completed"},"tags":[]},"outputs":[],"source":["class State(TypedDict):\n","    question: str\n","    context: List[Document]\n","    answer: str"]},{"cell_type":"code","execution_count":21,"id":"cc1ebcd3","metadata":{"execution":{"iopub.execute_input":"2025-05-08T08:16:09.19896Z","iopub.status.busy":"2025-05-08T08:16:09.198219Z","iopub.status.idle":"2025-05-08T08:16:09.203555Z","shell.execute_reply":"2025-05-08T08:16:09.202748Z"},"papermill":{"duration":0.014385,"end_time":"2025-05-08T08:16:09.204954","exception":false,"start_time":"2025-05-08T08:16:09.190569","status":"completed"},"tags":[]},"outputs":[],"source":["def retrieve(state: State):\n","    retrieved_docs = vector_store.similarity_search(state[\"question\"])\n","    return {\n","        \"question\": state[\"question\"],\n","        \"context\": retrieved_docs\n","    }\n","\n","\n","def generate(state: State):\n","    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n","    formatted_prompt = prompt.format(question=state[\"question\"], context=docs_content)\n","    response = llm.invoke(formatted_prompt)\n","    return {\"answer\": response.content}"]},{"cell_type":"code","execution_count":22,"id":"0c5122ac","metadata":{"execution":{"iopub.execute_input":"2025-05-08T08:16:09.219679Z","iopub.status.busy":"2025-05-08T08:16:09.219435Z","iopub.status.idle":"2025-05-08T08:16:09.223836Z","shell.execute_reply":"2025-05-08T08:16:09.222858Z"},"papermill":{"duration":0.013259,"end_time":"2025-05-08T08:16:09.225137","exception":false,"start_time":"2025-05-08T08:16:09.211878","status":"completed"},"tags":[]},"outputs":[],"source":["# Wrap your custom functions\n","retrieve_runnable = RunnableLambda(retrieve)\n","generate_runnable = RunnableLambda(generate)\n","\n","# Chain them\n","rag_chain = retrieve_runnable | generate_runnable"]},{"cell_type":"code","execution_count":23,"id":"553f9843","metadata":{"execution":{"iopub.execute_input":"2025-05-08T08:16:09.239954Z","iopub.status.busy":"2025-05-08T08:16:09.239306Z","iopub.status.idle":"2025-05-08T08:16:09.243646Z","shell.execute_reply":"2025-05-08T08:16:09.242763Z"},"papermill":{"duration":0.013093,"end_time":"2025-05-08T08:16:09.244941","exception":false,"start_time":"2025-05-08T08:16:09.231848","status":"completed"},"tags":[]},"outputs":[],"source":["def ask_rag_question(question: str) -> str:\n","    state = {\"question\": question, \"context\": [], \"answer\": \"\"}\n","    return rag_chain.invoke(state)[\"answer\"]"]},{"cell_type":"code","execution_count":24,"id":"224569db","metadata":{"execution":{"iopub.execute_input":"2025-05-08T08:16:09.259439Z","iopub.status.busy":"2025-05-08T08:16:09.259183Z","iopub.status.idle":"2025-05-08T08:16:10.232082Z","shell.execute_reply":"2025-05-08T08:16:10.230868Z"},"papermill":{"duration":0.981795,"end_time":"2025-05-08T08:16:10.233565","exception":false,"start_time":"2025-05-08T08:16:09.25177","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["MORAN stands for Multi-Object Rectified Attention Network. It is a network designed for general scene text recognition, particularly for irregular text. The network consists of a multi-object rectification network and an attention-based sequence recognition network. The rectification network helps to correct distorted text, making it easier for the recognition network to read.\n"]}],"source":["question = \"MORAN is stands for?\"\n","answer = ask_rag_question(question)\n","print(answer)"]},{"cell_type":"markdown","id":"719fd9df","metadata":{"papermill":{"duration":0.006828,"end_time":"2025-05-08T08:16:10.309204","exception":false,"start_time":"2025-05-08T08:16:10.302376","status":"completed"},"tags":[]},"source":["# Future Work\n","To improve and expand the current system, the following future enhancements are planned:\n","\n","* **PDF Upload Support** â€“ Allow users to upload their own documents for Q&A.\n","* **Persistent Vector Store** â€“ Replace in-memory storage with FAISS or Chroma for scalability.\n","* **Better Prompts** â€“ Use more dynamic prompts for improved answer quality.\n","* **User Feedback** â€“ Add ratings or comments to evaluate and refine responses.\n","* **LangGraph Integration** â€“ Explore more advanced workflows and multi-turn reasoning.\n","* **Web Interface** â€“ Deploy with Streamlit or Gradio for easier user interaction."]},{"cell_type":"markdown","id":"9947ab13","metadata":{"papermill":{"duration":0.00658,"end_time":"2025-05-08T08:16:10.322617","exception":false,"start_time":"2025-05-08T08:16:10.316037","status":"completed"},"tags":[]},"source":["# Thank You!\n","\n","This project was built as part of the GenAI Capstone.\n","\n","Iâ€™m proud of how far Iâ€™ve come. I am excited to grow further.\n","\n","Thank you for reviewing ScholarAI!"]},{"cell_type":"code","execution_count":null,"id":"72df77aa","metadata":{"papermill":{"duration":0.006489,"end_time":"2025-05-08T08:16:10.335845","exception":false,"start_time":"2025-05-08T08:16:10.329356","status":"completed"},"tags":[]},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"67e2c0e1","metadata":{"papermill":{"duration":0.006593,"end_time":"2025-05-08T08:16:10.349044","exception":false,"start_time":"2025-05-08T08:16:10.342451","status":"completed"},"tags":[]},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"sourceId":97258,"sourceType":"competition"},{"datasetId":1611656,"sourceId":2664123,"sourceType":"datasetVersion"}],"dockerImageVersionId":31012,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"},"papermill":{"default_parameters":{},"duration":64.087137,"end_time":"2025-05-08T08:16:13.542005","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-05-08T08:15:09.454868","version":"2.6.0"}},"nbformat":4,"nbformat_minor":5}